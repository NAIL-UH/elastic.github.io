<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ELASTIC: Efficient Once For All Iterative Search for Object Detection on Microcontrollers."> 
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ELASTIC: Efficient Once For All Iterative Search for Object Detection on Microcontrollers</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- Not sure what this is for-->
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="static/images/icon.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@2.0.4/dist/css/bulma-carousel.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@2.0.4/dist/js/bulma-carousel.min.js"></script>

  <script>
  document.addEventListener('DOMContentLoaded', function () {
    bulmaCarousel.attach('#results-carousel', {
      slidesToScroll: 1,
      slidesToShow: 1,
      loop: true,
      autoplay: false // Videos play automatically, so disable autoplay for carousel
    });
  });
  </script>
  
  <style>
    .cover-image img {
      max-width: 50%;
      height: auto;
      display: block;
      margin: 0 auto;
    }
    .structure-image img {
      max-width: 100%;
      height: auto;
      display: flex;
      margin: 0 auto;
      justify-content: center;
    }
    .carousel-container {
      padding: 0 0 60px 0;
      overflow: hidden;
    }
    
    .carousel-inner {
      overflow: visible;
    }
  </style>
  
</head>
<body>
<!---
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>
---->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ELASTIC: Efficient Once For All Iterative Search for Object Detection on Microcontrollers</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/thtran37/">Tony Tran</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://air.egr.uh.edu/">Qin Lin</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://binhu85.github.io/">Bin Hu</a><sup>*,2,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Cullen College of Engineering Research Computing at University of Houston</span>
            <span class="author-block"><sup>2</sup>Department of Electrical and Computer Engineering at University of Houston</span>
            <span class="author-block"><sup>3</sup>Department of Engineering Technology at University of Houston</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.21999"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.21999"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. Temporary placeholder -->
              <!--span class="link-block">
                <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ&ab_channel=RickAstley"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
            </span-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/NAIL-UH/ELASTIC"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section-->

  <!-- Paper Image. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-half-width">
      <div class="cover-image">
        <img src="static/images/historyannot.png" alt="Cover Image">
      </div>
    </div>
  </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Deploying high-performance object detectors on TinyML platforms poses significant challenges due to tight 
            hardware constraints and the modular complexity of modern detection pipelines. Neural Architecture Search (NAS) 
            offers a path toward automation, but existing methods either restrict optimization to individual modules, 
            sacrificing cross-module synergy, or require global searches that are computationally intractable.
          </p>
          <p>
            We propose ELASTIC (Efficient Once for All Iterative Search for Object Detection on Microcontrollers), 
            a unified, hardware-aware NAS framework that alternates optimization across modules (e.g., backbone, neck, and head) 
            in a cyclic fashion. ELASTIC introduces a novel Population Passthrough mechanism in evolutionary search that retains 
            high-quality candidates between search stages, yielding faster convergence, up to an 8% final mAP gain, and eliminates 
            search instability observed without population passthrough.
          </p>
          <p>
            In a controlled comparison, empirical results show ELASTIC achieves +4.75% higher mAP and 2× faster convergence than 
            progressive NAS strategies on SVHN, and delivers a +9.09% mAP improvement on PascalVOC given the same search budget. 
            ELASTIC achieves 72.3% mAP on PascalVOC, outperforming MCUNET by 20.9% and TinyissimoYOLO by 16.3%. When deployed on 
            MAX78000/MAX78002 microcontrollers, ELASTIC-derived models outperform Analog Devices’ TinySSD baselines, reducing energy 
            by up to 71.6%, lowering latency by up to 2.4×, and improving mAP by up to 6.99 percentage points across multiple datasets.
          </p>
        </div>
      </div>
    </div>
     </div>
    <!--/ Abstract. -->
</section>

  <section class="section">
  <div class="container is-max-desktop">
    <!-- Paper Structure. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview of ELASTIC Framework</h2>
          <div class="structure-image">
            <img src="static/images/method.png" alt="Block Diagram">
          </div>
          <p style="text-align: left;">
            Our method begins with a pretrained supernet and performs iterative neural architecture search by alternating 
            optimization between the backbone and head. The Population Passthrough mechanism ensures continuity by retaining 
            top-performing candidates across module alternations. 
          </p>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
  </section>
  
  <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparison of ELASTIC Framework</h2>
        <div class="table-container">
          <figure class="has-text-centered" id="tab-mcunet">
            <figcaption class="has-text-weight-semibold is-size-6" style="margin-bottom:0.5rem;">
              <strong>Comparison of ELASTIC with TinyissimoYOLO and MCUNET on the full PascalVOC dataset, considering all object classes and counts.</strong>
              ELASTIC achieves a 20.9% mAP boost over MCUNET, 4.0% over MCUNetV2, and a 16.3% over TinyissimoYOLO’s best performing model, while discovering a model with significantly fewer MACs, enabling faster inference on microcontrollers.
            </figcaption>

            <table class="table is-striped is-hoverable is-fullwidth is-narrow" style="font-size:0.9rem;">
              <thead>
                <tr>
                  <th>Method</th>
                  <th>MACs</th>
                  <th>&darr;MACs</th>
                  <th>Params</th>
                  <th>VOC mAP</th>
                  <th>&uarr;mAP</th>
                </tr>
              </thead>
              <tbody class="has-text-centered">
                <tr><td>TY: 20-3-88</td><td>32M</td><td>90.7%</td><td>0.58M</td><td>53%</td><td>+30%</td></tr>
                <tr><td>TY: 20-7-88</td><td>44M</td><td>87.2%</td><td>0.58M</td><td>47%</td><td>+24%</td></tr>
                <tr><td>TY: 20-3-112</td><td>54M</td><td>84.3%</td><td>0.89M</td><td>56%</td><td>+33%</td></tr>
                <tr><td>TY: 20-7-112</td><td>70M</td><td>79.6%</td><td>0.91M</td><td>53%</td><td>+30%</td></tr>
                <tr><td>TY: 20-3-224</td><td>218M</td><td>36.4%</td><td>3.34M</td><td>23%</td><td>+0%</td></tr>
                <tr><td>MCUNet</td><td>168M</td><td>51.0%</td><td>1.2M</td><td>51.4%</td><td>+28.4%</td></tr>
                <tr><td>MCUNetV2-M4</td><td>172M</td><td>49.9%</td><td>0.47M</td><td>64.6%</td><td>+41.6%</td></tr>
                <tr><td>MCUNetV2-H7</td><td>343M</td><td>0%</td><td>0.67M</td><td>68.3%</td><td>+45.3%</td></tr>
                <tr class="has-background-primary-light">
                  <td><strong>ELASTIC (OURS)</strong></td>
                  <td><strong>86M</strong></td>
                  <td><strong>74.9%</strong></td>
                  <td><strong>1.36M</strong></td>
                  <td><strong>72.3%</strong></td>
                  <td><strong>+49.3%</strong></td>
                </tr>
              </tbody>
            </table>

            <p class="is-size-7 has-text-grey">
              Notes: “MACs” denotes multiply–accumulate operations (lower is better). “&darr;MACs” and “&uarr;mAP” indicate relative change w.r.t. the MCUNetV2-H7 reference.
            </p>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

        <!-- Simulation -->
    <section class="section">
        <div class="container is-max-desktop">
        <h2 class="title is-3">Results from Gazebo Simulations</h2>
    
        <h3 class="title is-4">Simulation 1: Two-Robot Leader-follower Formation in Oval Track Environment</h3>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column  has-text-centered">
            <h5><b>With CBF</b><br>(In-View Safe Tracking)<br></h5>
            <h5 style="font-size:30px;">✅</h5>
            <video autoplay controls muted loop playsinline width="80%">
              <source src="static/videos/2Oval-with.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column  has-text-centered">
            <h5><b>Without CBF</b><br>(Out-of-View Crash)<br></h5>
            <h5 style="font-size:30px;">❌</h5>
            <video autoplay controls muted loop playsinline width="80%">
              <source src="static/videos/2Oval-without.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="content has-text-justified">
          <img src="static/images/2Oval.png" />
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
              <p>
                The first simulation evaluates the proposed perception-aware safe leader-follower control scheme in an oval track environment and is compared to 
                a baseline that only applies the leader-follower formation controller. The system is evaluated in three stages: (i) an initial straight-line formation
                where the follower remains directly behind the leader, (ii) a formation transition where the follower moves to the side of the leader, introducing a
                conflict between control objectives and safety constraints, and (iii) a return to the initial straight-line formation. The proposed approach successfully
                resolve this conflict by adaptively activating the CBF-QP safety filter to maintain the leader within its FOV, while the baseline system fails to do so, resulting
                in the loss of the leader causing instability. These results demonstrate the control scheme's effectiveness in maintaining formation and safety constraints.
                
              </p>
            </tr>
          </table>
        </div>
    
        <h3 class="title is-4">Simulation 2: Two-robot Leader-follower Formation in Residence Environment</h3>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column  has-text-centered">
            <h5><b>With CBF</b><br>(In-View Safe Tracking)<br></h5>
            <h5 style="font-size:30px;">✅</h5>
            <video autoplay controls muted loop playsinline width="80%">
              <source src="static/videos/2Outdoor-with.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column  has-text-centered">
            <h5><b>Without CBF</b><br>(Out-of-View Crash)<br></h5>
            <h5 style="font-size:30px;">❌</h5>
            <video autoplay controls muted loop playsinline width="80%">
              <source src="static/videos/2Outdoor-without.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="content has-text-justified">
          <img src="static/images/2Outdoor.png" />
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
              <p>
                The second simulation evaluates the generalization capability of the proposed perception-aware control scheme and the DNN-based estimator, the second experiment is conducted
                in an unseen outdoor residence environment. Despite being trained exclusively in the oval track setting, the estimator is able to generalize its environement effectively, providing
                an accurate estimation that enables the proposed controller to maintain desired formation and safety constraints. The baseline approach, fails to adapt to the environmental differences,
                leading to the loss of the leader causing instability. This simulation validates the robustness of the proposed estimator to ensure a robust and reliable leader-follower formation.
              </p>
            </tr>
          </table>
        </div>
    
        <h3 class="title is-4">Simulation 3: Three-robot Leader-follower Formation in an Oval Track Environment</h3>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column  has-text-centered">
            <h5><b>With CBF</b><br>(In-View Safe Tracking)<br></h5>
            <h5 style="font-size:30px;">✅</h5>
            <video autoplay controls muted loop playsinline width="80%">
              <source src="static/videos/3Oval-with.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column  has-text-centered">
            <h5><b>Without CBF</b><br>(Out-of-View Crash)<br></h5>
            <h5 style="font-size:30px;">❌</h5>
            <video autoplay controls muted loop playsinline width="80%">
              <source src="static/videos/3Oval-without.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="content has-text-justified">
          <img src="static/images/3Oval.png" />
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
              <p>
                The third experiment evaluates the proposed control scheme in a three-robot leader-follower formation within the oval track environment. In this setting,
                the last robot is susceptible to the safety violations due to the error propagation and disturbances caused by the preceding robots. This demonstration shows
                that the robot not utilizing the proposed approach fails almost immediately in Stage 1. In contrast, the proposed approach successfully preserves the safety.
              </p>
            </tr>
          </table>
        </div>

        <!-- Experiment. -->
        <h2 class="title is-3">Experiments with Two ROSbot Pro Mobile Robots</h2>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column  has-text-centered">
            <h5><b>With CBF</b><br>(In-View Safe Tracking)<br></h5>
            <h5 style="font-size:30px;">✅</h5>
            <video autoplay controls muted loop playsinline width="80%">
              <source src="static/videos/Exp1_with.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column  has-text-centered">
            <h5><b>Without CBF</b><br>(Out-of-View Unsafe Tracking)<br></h5>
            <h5 style="font-size:30px;">❌</h5>
            <video autoplay controls muted loop playsinline width="80%">
              <source src="static/videos/Exp1_without.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="content has-text-justified">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
              <p>
               Experiments using two ROSbot Pro 2 robots confirm that our perception-aware leader-follower control scheme consistently keeps the leader within the follower's field of view, ensuring safe, in-view tracking. In contrast, the traditional formation control strategy without CBF loses the leader, resulting in unsafe, out-of-view tracking. 
                Note that these experiments employed an OptiTrack Motion Capture System for state estimation as a proof-of-concept, with future tests incorporating DNN and DBB-based estimators.
              </p>
            </tr>
          </table>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
      </div>
    </div>
      <div class="columns is-centered">
        <div class="column has-text-centered">
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{tran2025facets,
      title={FACETS: Efficient Once-for-all Object Detection via Constrained Iterative Search}, 
      author={Tony Tran and Bin Hu},
      year={2025},
      eprint={2503.21999},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.21999},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
                    href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                    International</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
